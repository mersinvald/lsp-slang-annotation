\chapter{Implementation}
\label{chap:impl}

The project was decided to be named Akkadia, trivia of this name can be found in Appendix A \ref{trivia:akkadia}.
This chapter describes implementation specifics of Akkadia.

\section{Instruments}
\label{sec:impl:instruments}

\subsection{Programming Language}
As the language with which researcher is most acquiented with is Rust, a modern programming language that aims
to provide zero-cost abstractions, memory and thread safety and blazing speed,
with a decent coding experience at the same time, it was decided to implement this work in it.
Becides an author's favor, Rust can provide the following features that are useful for a prototype ought to be implemented in a limited
amount of time:
\begin{itemize}
    \item{Rich type system for complex abstractions and code reuse}
    \item{Compile-time error catching for reduced debugging time compared to the other non-GC languages}
    \item{Deterministic resource management through RAII}
    \item{
        Convenient error handling through sum types that solves the problem of raising unextected (and, hence, unhandled) exceptions from foreigh codebase,
        because errors are returned from function as a part of Result enum, and at the same time makes returning errors practical compared to C/C++ approach.
        In virtue of Result concept, user of a function that can fail can not use the data avoiding error handling.
        Thus Rust helps to avoid problems which are inflicting noticeable damage caused by hard-to-debug issues such as a Null Pointer Exception in non-null-safe languages
        or the use of uninitialized memory.
    }
\end{itemize}

\subsection{IDE}
The main part of a developers tooling nowadays is an IDE capable of providing code analysis, hints, refactoring methods,
and other integrated unilities to speed up development process. For an author there were a choice between two editors/IDEs:
\textbf{Visual Studio Code} and \textbf{Intellij Idea} with respective plugins for the Rust language support.
Both of them had their pros and cons at the moment of writing this paper:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Visual Studio Code} & \textbf{Intellij Idea} \\
        \hline
        \makecell{small response times, \\ lower memory footprint} & \makecell{bit slow, \\ needs a powerful workstation} \\
        \hline
        \makecell{bad support of \\Cargo Workspace project} & \makecell{supports everything that \\ can be built with Cargo} \\
        \hline
        \makecell{poor code analysis} & \makecell{type info and linter through plugin} \\
        \hline
        \makecell{good cargo integration \\ for the whole project} & \makecell{can only show errors \\ in the current file} \\
        \hline
        \makecell{stagnation in plugins development} & \makecell{active plugin development} \\
        \hline
    \end{tabular}
    \caption{Code editors comparison}
    \label{table:impl:vscode_vs_idea}
\end{table}

The issues of both code editing solutions are to be solved by Rust Language Server (with adding some more problems), which is currentry
under heavy development and on the oppinion of the author is not ready to be integrated into the everyday development practice.

\subsection{Donor codebase}
To not to get drown under tons of work, it was decided to choose an existing open-source
language server as an implementational basis.

Rust ecosystem have to provide an open-source language server written in Rust for Rust,
which can be forked and adopted for the respective use in SLang ecosystem.
RLS (Rust Language Server) has a well-tested architectural basis to employ as a necessary basement level,
which would be a long work to implement and debug otherwise.
Parts of RLS which are useful for the SLS implementation:
\begin{itemize}
    \item Language Server Protocol implementation
    \item VFS(Virtual File System)
    \item Convenient API to accept and dispatch Language Server messages
\end{itemize}

\section{Preparing RLS codebase}

The first step of implemeenting Slang Language Sever using RLS as a basis is to prune all Rust language-specific functionality,
that is connected to the Rust compiler or other Rust code analysis tools, such as the \textbf{racer} tool, responsible for autocomplete hints.
That reduced the code base almost in a half, leaving only Language Server service code and placeholders for future functions implementation.

After the first step, the project ended up with the following structure:
\begin{itemize}
    \item{
        \textbf{akkadia}: the main language server source tree:
        \begin{itemize}
        \item{
            \textbf{actions}: control messages definitons and corresponding actions implementations:
            \begin{itemize}
            \item{\textbf{notifications}: messages that do not require response}
            \item{\textbf{requests}: messages that do require response}
            \end{itemize}
        }
        \item{
            \textbf{server}: io and the main server loop
            \begin{itemize}
            \item{\textbf{service}: main loop and a glue to connect all components}
            \item{\textbf{io}: input-output handling}
            \end{itemize}
        }
        \item{\textbf{test}: integration testing}
        \end{itemize}
    }
    \item{
        \textbf{akkadia-span}: types for identifying code spans/ranges
        \begin{itemize}
        \item{\textbf{compiler}: conversion from the compiler span/range types to the Akkadia types}
        \item{\textbf{main}: type definitions}
        \end{itemize}
    }
    \item{
        \textbf{akkadia-vfs}: virtual file system to keep file tree and file cache
    }
\end{itemize}

\section{Module System}
Akkadia is designed to be extensible with external (out of source tree) modules,
so the inter-module IPC should be language-agnostic, therefore the most easy-to-employ and probably
the most flexible solution is to use serialized data on the input and the output of the module interface.

Defined in Rust types, the \textbf{Input} and \textbf{Output} of a module looks as follows:
\begin{minted}{rust}
// A trait defining module parameters
pub trait Params: serde::Serialize + serde::Deserialize<'static> {}
// A trait defining module response data
pub trait Response: serde::Serialize + serde::Deserialize<'static> {}
\end{minted}
This basically means that anything that can be serialized and deserialized in an arbitary format
supported by serde-rs\ref{crates::serde}, can be a module argument or a module return type.
These are just convenience flag-traits, that do now provide their own methods.

Consequetively, there should be a data type that implements one of those traits (or both).
In the current prototype, all modules have full access to the Language Server state, so it is
provided to them via IPC interface, in serialized data format:
\begin{minted}{rust}
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct State {
    // Virtual File System containing the source tree
    vfs: Vfs,
    // Current project path
    path: PathBuf,
    // Method on which the module was invoked
    method: Method,
    // A mutable responce list
    responses: Vec<Response>
}

impl Params for State {}
impl Response for State {}

\end{minted}

The first fields are self-explanatory, but the method and response types are a bit complicated.
To start with the \textbf{Method} type, it is to be said that it is a enumeration containing
different method names as the data supplied with the method.
\begin{minted}{rust}
pub enum Method {
    Completion(Span),
    <...>
    ExitNotification
}
\end{minted}

Method itself is an aggregation of both \textbf{Notifications}
and \textbf{Requests}. Intuitively, the only difference is that the first ones do not require \textbf{Response}.
\textbf{Response} is a enum with variants containing data too,
the specific data structures that are expected to be returned by the respective methods.
\begin{minted}{rust}
pub enum Response {
    Completion(Hint),
    <...>
}
\end{minted}

All of that is used in the \textbf{Module} trait which defines the necessary IO methods for the IPC
communication to the modules.
\begin{minted}{rust}
pub trait Module {
    type Params: Params;
    type Response: Response;
    type Error;
    fn call(params: Self::Params) -> Result<Self::Response, Self::Error>;
}
\end{minted}
This is a very generic trait that can be implemented for basically any module IPC IO needs,
be it basic stdin-stdout, TCP socket connection or even some wicked blockchain-based external service.
The specific implementations of the \textbf{Module} trait define the built-in module types
that can be loaded via config file and used with no Language Server code hacking necessary.
This built-ins are not easily extendable, but extendability is possible indeed through
shared libraries, for example, as the users of the Module trait inside a Langage Server
are not aware of the concrete types because of extensible use of \textbf{Box<Trait>} pattern,
that provides dynamic dispatch capabilities for any types implementing the \textbf{Method} trait.

\newpage
To summarize modules data flow, the implementation of a very basic language server module
that accepts json-formatted state, reacts to a Completion method and returns a "hello world" hint would be
\begin{minted}{rust}
extern crate serde;
extern crate serde_json as json;
extern crate akkadia;

use std::io::{
    stdin,
    stdout,
};

use akkadia::{
    State,
    Method,
    Response,
    Hint,
};

fn main() {
    // Read serialized State
    let mut state: State = json::from_reader(stdin())
        .unwrap();

    // Add the "Hello World" hint to responses
    match *state.method {
        Method::Completion(span) => {
            state.responses.push(Hint::new(span, "Hello World!"))
        },
        _ => () /* do nothing */
    }

    // Write State back to the stdout
    json::to_writer(stdout(), &state)
        .unwrap();
}
\end{minted}

\section{Dispatcher and the Module Registry}

The core component of the module system that is accountable for routing data to the modules is the
\textbf{Dispatcher}. It works in close communication with the \textbf{Module Registry}, a part of system responsible for
holding a list of modules with their characteristics, loading lists of modules and building the chains of modules.

Firsty, when the Language Server accepts a request, it invokes the \textbf{Dispatcher}
\begin{minted}{rust}
impl Server {
    fn loop(mut self) {
        loop {
            if let Err(e) = self.step() {
                eprintln!("{}", e);
            }
        }
    }

    fn step(&mut self) -> Result<(), Error> {
        let message = read_message(&mut self.input)?;
        let state = self.dispatcher.handle(message)?;
        Ok(write_results(&mut self.output, state.responses))
    }
}
\end{minted}

The dispatcher is connected to the \textbf{Module Registry} that is able to supply
all the necessary information about modules and their sequences.
\begin{minted}{rust}
struct Dispatcher {
    registry: Registry,
}

struct Registry {
    modules: Vec<Box<Module>>
}
\end{minted}

As it can be recalled from the previous section, modules may influence each other in 2 ways:
override and build composite-like chained structures. In the first case, an override is
transparent for the \textbf{Dispatcher}, as the overriden modules
get unregistered inside the \textbf{Module Registry}.
The later case is not though, so it is a place to introduce the \textbf{Module Chain} abstraction,
which is basically a Rust iterator over modules:

\begin{minted}{rust}
impl Registry {
    // Returns an iterator over module chains
    fn get_method_handlers(method: &Method) -> impl Iterator<Item=impl Iterator<&Method>> {
        /* build method handlers chains based on inheritance rules */
    }
}
\end{minted}

Once the chain list is built, \textbf{Dispatcher} uses that info to run the modules:
\begin{minted}{rust}
impl Dispatcher {
    // Returns an iterator over module chains
    fn handle(&self, message: Message) -> Result<State, Error> {
        let method = message.into_method();

        let empty_state = self.make_empty_state(&method);
        let final_state = empty_state.clone();

        for chain in self.registry.get_method_handlers(method) {
            let chain_state = empty_state.clone()
            for module in chain {
                chain_state = module.run(chain_state)?;
            }
            final_state.merge(&chain_state)
        }

        Ok(final_state)
    }
}
\end{minted}

So, the dispatcher has two kinds of state handling: it merges state of indepentent chains
and applies a replacement technique for the state that is propagated through a single chain.
At the same time, all the chains accept the initial state unchanged and different chains
can not affect each others data.

\section{Autocomplete}
\label{sec:impl:ls_mod:autocomplete}
Autocomplete is the most crucial and most used development helper, that is crucial for any code editor to have.
It is a good self-contained example of the Language Server module implementation, that employs
both the module system and the SLang compilers Semantic Representation of the source tree.

The Semantic Representation of a language contains the source code entities such as \textbf{Units},
\textbf{Routines} and \textbf{Variables}, as well as the scope information, that can be used for autocompletion.
Besides the dynamic entities, that are different from codebase to codebase, the language itself
has \textbf{keywords}, which are to be autocompleted too.

Therefore the autocompletion high-level algorithm will look as follows:
\begin{itemize}
\item Accept the user input
\item Find matches in keyword list
\item Find matches in entities in the current and parent scopes
\item Build a list of hints
\item Return to the user
\end{itemize}

Matches search is a straightforward task, but it might be quite slow on big code bases.
There are several methods to approach the speed problem, both on compiler interface
and on a matches search ends. First of all, it is not feasible to re-run parser every time user
might need an autocompletion hint, so the parser output shoud be cached and updated as a background task
that would not block the hint generation process. This way the parser re-runs should not be noticeable by user
at a level beyond a small lag in completeness of autocomplete hints list.

The next method approaching the matching speed includes extensive use of an indexed cache and
an appropriate algorithm for the matching task, particularly an \textbf{Aho-Corasick} algorithm
that is known to be used in several autocompletion tools, such as Rust`s \textbf{racer}.

Once built, the Aho-Corasick index may be used until the next change in the source tree to perform
rapid searches.
Aho-Corasick automaton is implemented in rust crate \textbf{aho-corasick}, that is imported in
the autocompletion module implementation.

\section{Language Server control utility\\ library}
\label{sec:impl:ls_control_api}
Testing and Language Server usage as utility outside from within of client-editor Language Server Protocol client would require
a CLI tool, that would be able to act like and aditor and provide an appropriate interface to run single or batch tasks.

Basically, that is a simple language server client that can connect to the Akkadia and then perform the messages exchange via jsonrpc
like a real Language Server Client in the editor would do.
It have been imlemented for integration testing of the Language Server.